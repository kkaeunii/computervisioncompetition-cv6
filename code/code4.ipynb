{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2165d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.12)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.19.4)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 설치합니다.\n",
    "# PyTorch 기반 SOTA 이미지 모델 라이브러리 ; resnet34 불러오기 위함\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef0fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import timm # 모델\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image # 이미지 입출력\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decfbda",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6b7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ID  target\n",
      "0  002f99746285dfdd.jpg      16\n",
      "1  008ccd231e1fea5d.jpg      10\n",
      "2  008f5911bfda7695.jpg      10\n",
      "3  009235e4c9c07af5.jpg       4\n",
      "4  00b2f44967580c74.jpg      16\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1570 entries, 0 to 1569\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      1570 non-null   object\n",
      " 1   target  1570 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.7+ KB\n",
      "None\n",
      "[16 10  4  5 15 14  9 13  7 11  2  8 12  3  0  1  6]\n"
     ]
    }
   ],
   "source": [
    "# train.csv 로드 & 기본 구조\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "print(train_df.head())\n",
    "print(train_df.info())\n",
    "print(train_df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71ff505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 클래스 분포 확인\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = train_df['target'].value_counts().sort_index()\n",
    "counts.plot(kind='bar', figsize=(10,5))\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7caa35f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1570/1570 [00:00<00:00, 21248.15it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미지 경로 확인 & 크기 분포\n",
    "heights, widths = [], []\n",
    "for img_id in tqdm(train_df['ID']):\n",
    "    path= f\"../data/train/{img_id}\"\n",
    "    with Image.open(path) as img:\n",
    "        w, h = img.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "        \n",
    "plt.hist(widths, bins=30, alpha=0.5, label='Width')\n",
    "plt.hist(heights, bins=30, alpha=0.5, label='Height')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c445328",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe1f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, math\n",
    "\n",
    "SEED = 42\n",
    "N_CLASS = 17\n",
    "\n",
    "# 프로젝트 구조 맞춤 (지금 노트북이 code/ 폴더에 있으니 한 단계 위의 data/)\n",
    "DATA_DIR = \"../data\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_CSV  = os.path.join(DATA_DIR, \"sample_submission.csv\")  # 테스트 ID용(csv 이름 다르면 바꿔)\n",
    "\n",
    "# GPU 메모리 기준으로 preset 고르기 (원하면 고정값으로 덮어써도 됨)\n",
    "def pick_preset():\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"img_size\": 320, \"batch_size\": 32, \"accum\": 1}\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "    if total < 10:      # ~8GB\n",
    "        return {\"img_size\": 320, \"batch_size\": 32, \"accum\": 1}\n",
    "    elif total < 18:    # ~12-16GB\n",
    "        return {\"img_size\": 384, \"batch_size\": 32, \"accum\": 1}\n",
    "    else:               # 24GB+\n",
    "        return {\"img_size\": 448, \"batch_size\": 64, \"accum\": 1}\n",
    "\n",
    "preset = pick_preset()\n",
    "\n",
    "CFG = {\n",
    "    # device\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # data\n",
    "    \"train_csv\": TRAIN_CSV,\n",
    "    \"train_img_dir\": TRAIN_IMG_DIR,\n",
    "\n",
    "    # model\n",
    "    \"model_name\": \"convnext_tiny\",  # 'resnet34', 'efficientnet_b0', 'convnext_tiny' 등 timm 이름\n",
    "    \"num_classes\": N_CLASS,\n",
    "\n",
    "    # training\n",
    "    \"img_size\": preset[\"img_size\"],   # 384 권장\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": preset[\"batch_size\"],\n",
    "    \"grad_accum\": preset[\"accum\"],    # OOM나면 2~4로\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_epochs\": 3,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"use_amp\": True,\n",
    "\n",
    "    # dataloader\n",
    "    \"num_workers\": max(2, (os.cpu_count() or 4) // 2),\n",
    "    \"pin_memory\": True,\n",
    "    \"persistent_workers\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9be50",
   "metadata": {},
   "source": [
    "# Transform(Albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadced5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = IMG_SIZE = CFG['img_size']\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# train : 강건성 확보\n",
    "train_tf = A.Compose([\n",
    "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
    "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=0, value=[0,0,0]),\n",
    "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6, 1.0), ratio=(0.7, 1.3), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5, p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3,7), p=1.0),\n",
    "        A.MedianBlur(blur_limit=5, p=1.0),\n",
    "    ], p=0.2),                 # 평가셋 블러 대응\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.GaussNoise(var_limit=(5.0, 25.0), p=1.0),\n",
    "    ], p=0.2),                 # 압축/노이즈 내성\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(10,15,10, p=1.0),\n",
    "        A.RandomBrightnessContrast(0.2,0.2, p=1.0),\n",
    "    ], p=0.4),                 # 색/밝기 흔들림\n",
    "    A.CoarseDropout(max_holes=1, max_height=0.25, max_width=0.25, p=0.25),  # RandomErasing 유사\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 소수 클래스용 강 증강(필요 없으면 안 써도 됨)\n",
    "strong_train_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
    "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=0, value=[0,0,0]),\n",
    "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.6, 1.0), ratio=(0.7, 1.35), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=7, p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(5,9), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "    ], p=0.25),\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(quality_lower=50, quality_upper=95, p=1.0),\n",
    "        A.GaussNoise(var_limit=(10.0, 40.0), p=1.0),\n",
    "    ], p=0.25),\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(10,15,10, p=1.0),\n",
    "        A.RandomBrightnessContrast(0.2,0.2, p=1.0),\n",
    "    ], p=0.4),\n",
    "    A.CoarseDropout(max_holes=1, max_height=0.25, max_width=0.25, p=0.3),\n",
    "    A.Normalize(mean=MEAN, std=STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# valid/test : 안정성 우선\n",
    "valid_tf = A.Compose([\n",
    "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
    "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=0, value=[0,0,0]),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17f17df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    csv:        CSV 경로 (기본 컬럼명: ID, target)\n",
    "    img_dir:    이미지 폴더 경로\n",
    "    transform:  기본(약) 증강 (albumentations)\n",
    "    transform_strong: 강 증강 (albumentations) — minority_set에만 적용\n",
    "    minority_set: 강증강을 적용할 라벨 집합(e.g., {3, 7, 12}); None이면 미사용\n",
    "    has_label:  True면 학습/검증용, False면 테스트용\n",
    "    id_col:     파일명 컬럼명\n",
    "    label_col:  라벨 컬럼명\n",
    "    img_ext:    CSV의 ID에 확장자가 없을 때 붙일 확장자 (예: \".jpg\")\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv,\n",
    "        img_dir,\n",
    "        transform=None,\n",
    "        transform_strong=None,\n",
    "        minority_set=None,\n",
    "        has_label=True,\n",
    "        id_col=\"ID\",\n",
    "        label_col=\"target\",\n",
    "        img_ext=\".jpg\",\n",
    "    ):\n",
    "        self.df = pd.read_csv(csv).reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.transform_strong = transform_strong\n",
    "        self.minority_set = set(minority_set) if minority_set is not None else None\n",
    "        self.has_label = has_label\n",
    "        self.id_col = id_col\n",
    "        self.label_col = label_col\n",
    "        self.img_ext = img_ext\n",
    "\n",
    "        # ID에 확장자가 이미 포함되어 있는지 자동 판별\n",
    "        first_id = str(self.df[self.id_col].iloc[0])\n",
    "        self._id_has_ext = first_id.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\"))\n",
    "\n",
    "        # 안전장치: 테스트 모드인데 라벨 컬럼이 있으면 무시\n",
    "        if not self.has_label and self.label_col in self.df.columns:\n",
    "            self.df = self.df.drop(columns=[self.label_col])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _make_path(self, name: str) -> str:\n",
    "        fname = name if self._id_has_ext else f\"{name}{self.img_ext}\"\n",
    "        return os.path.join(self.img_dir, fname)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name = str(row[self.id_col])\n",
    "        path = self._make_path(name)\n",
    "\n",
    "        # --- 이미지 읽기 + EXIF 회전 보정 (PIL에서 보정 후 넘파이 변환) ---\n",
    "        try:\n",
    "            with Image.open(path) as pil:\n",
    "                pil = ImageOps.exif_transpose(pil)  # <-- PIL.Image에 대해 호출해야 함\n",
    "                pil = pil.convert('RGB')\n",
    "                img = np.array(pil)  # HWC, RGB\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\") from e\n",
    "\n",
    "        # --- transform 선택(소수 클래스라면 강증강) ---\n",
    "        t = self.transform\n",
    "        if self.has_label:\n",
    "            target = int(row[self.label_col])\n",
    "            if (self.transform_strong is not None) and (self.minority_set is not None) and (target in self.minority_set):\n",
    "                t = self.transform_strong\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        if t is not None:\n",
    "            img = t(image=img)[\"image\"]  # tensor (C,H,W)\n",
    "\n",
    "        if self.has_label:\n",
    "            return img, target\n",
    "        else:\n",
    "            # 테스트셋은 식별자도 같이 반환하면 편함\n",
    "            return img, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213b19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def _metrics_from_logits(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    acc = (preds == targets).float().mean().item()\n",
    "    f1  = f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "    return acc, f1\n",
    "\n",
    "def train_one_epoch(\n",
    "    loader, model, optimizer, loss_fn, device,\n",
    "    scheduler=None,                 # Cosine 등 epoch/batch 스케줄러 아무거나\n",
    "    scaler=None,                    # torch.cuda.amp.GradScaler() 쓰면 혼합정밀\n",
    "    grad_accum_steps=1,             # 메모리 부족 시 >1 로\n",
    "    max_grad_norm=1.0,              # 클리핑 미사용이면 None\n",
    "    use_amp=True                    # True면 autocast 사용\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    # 전체 예측/정답 모음 (F1 계산용)\n",
    "    all_preds, all_tgts = [], []\n",
    "\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (images, targets) in enumerate(pbar, 1):\n",
    "        images  = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(use_amp and torch.cuda.is_available())):\n",
    "            logits = model(images)\n",
    "            loss = loss_fn(logits, targets) / grad_accum_steps  # 누적학습 고려\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if step % grad_accum_steps == 0:\n",
    "            if max_grad_norm is not None:\n",
    "                if scaler is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # 스케줄러가 배치 단위(step)일 경우\n",
    "            if scheduler is not None and getattr(scheduler, \"step_per_batch\", False):\n",
    "                scheduler.step()\n",
    "\n",
    "        # 로그 집계\n",
    "        bs = images.size(0)\n",
    "        running_loss += loss.item() * bs * grad_accum_steps  # 복원\n",
    "        n_samples += bs\n",
    "\n",
    "        batch_preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        batch_tgts  = targets.detach().cpu().numpy()\n",
    "        all_preds.extend(batch_preds)\n",
    "        all_tgts.extend(batch_tgts)\n",
    "\n",
    "        pbar.set_description(f\"loss {loss.item()*grad_accum_steps:.4f}\")\n",
    "\n",
    "    # 에폭 종료 후 지표\n",
    "    epoch_loss = running_loss / max(1, n_samples)\n",
    "    epoch_acc  = accuracy_score(all_tgts, all_preds)\n",
    "    epoch_f1   = f1_score(all_tgts, all_preds, average='macro')\n",
    "\n",
    "    # 스케줄러가 에폭 단위일 경우\n",
    "    if scheduler is not None and not getattr(scheduler, \"step_per_batch\", False):\n",
    "        # 일부 스케줄러는 val_metric을 받기도 함(예: ReduceLROnPlateau)\n",
    "        try:\n",
    "            scheduler.step(epoch_f1)\n",
    "        except TypeError:\n",
    "            scheduler.step()\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"train_acc\":  epoch_acc,\n",
    "        \"train_f1\":   epoch_f1,\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(loader, model, loss_fn, device, return_probs=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    all_preds, all_tgts = [], []\n",
    "    all_probs = [] if return_probs else None\n",
    "\n",
    "    for images, targets in tqdm(loader, leave=False):\n",
    "        images  = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        bs = images.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        n_samples += bs\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_tgts.extend(targets.cpu().numpy())\n",
    "        if return_probs:\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    val_loss = running_loss / max(1, n_samples)\n",
    "    val_acc  = accuracy_score(all_tgts, all_preds)\n",
    "    val_f1   = f1_score(all_tgts, all_preds, average='macro')\n",
    "\n",
    "    out = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\":  val_acc,\n",
    "        \"val_f1\":   val_f1,\n",
    "    }\n",
    "    if return_probs:\n",
    "        out[\"val_probs\"] = np.concatenate(all_probs, axis=0)\n",
    "        out[\"val_targets\"] = np.array(all_tgts)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c456",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f9189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 비율 유지 + 정사각 패딩 → 학습에선 크롭으로 크롭 노이즈 내성\n",
    "trn_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=[0,0,0]),\n",
    "    A.RandomResizedCrop(img_size, img_size, scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5, p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(3,7), p=1.0),\n",
    "        A.MedianBlur(blur_limit=5, p=1.0),\n",
    "    ], p=0.15),  # 평가셋 블러 대응\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=95, p=1.0),\n",
    "        A.GaussNoise(var_limit=(5.0, 25.0), p=1.0),\n",
    "    ], p=0.15),\n",
    "    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 소수 클래스용 강증강(강도 ↑): scale 더 낮추고, 색/erasing 추가\n",
    "strong_train_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=[0,0,0]),\n",
    "    A.RandomResizedCrop(img_size, img_size, scale=(0.6, 1.0), ratio=(0.70, 1.35), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=7, p=1.0),\n",
    "        A.GaussianBlur(blur_limit=(5,9), p=1.0),\n",
    "        A.MedianBlur(blur_limit=7, p=1.0),\n",
    "    ], p=0.25),\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(quality_lower=50, quality_upper=95, p=1.0),\n",
    "        A.GaussNoise(var_limit=(10.0, 40.0), p=1.0),\n",
    "    ], p=0.25),\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(10,15,10, p=1.0),\n",
    "        A.RandomBrightnessContrast(0.2,0.2, p=1.0),\n",
    "    ], p=0.4),\n",
    "    A.CoarseDropout(max_holes=1, max_height=0.25, max_width=0.25, p=0.3),\n",
    "    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 검증/테스트: 비율 유지 + 센터크롭(혹은 패딩만) → 변동 최소화\n",
    "tst_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=img_size),\n",
    "    A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=[0,0,0]),\n",
    "    A.CenterCrop(img_size, img_size, p=1.0),\n",
    "    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82469f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# 클래스 비율 유지하며 train/val 분리\n",
    "trn_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size = 0.2, # 8:2 분리\n",
    "    stratify = df['target'], # 클래스 비율 유지\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 임시 csv 저장\n",
    "trn_df.to_csv('../data/train_split.csv', index=False)\n",
    "val_df.to_csv('../data/val_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72398741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256 314 3140\n"
     ]
    }
   ],
   "source": [
    "# minority_set 로드 (그대로 OK)\n",
    "minority_path = '/root/cv_data/minority_set.txt'\n",
    "if os.path.exists(minority_path):\n",
    "    with open(minority_path, 'r') as f:\n",
    "        txt = f.read().strip()\n",
    "    minority_set = set(map(int, txt.split(','))) if txt else set()\n",
    "else:\n",
    "    minority_set = set()\n",
    "\n",
    "# Dataset 생성\n",
    "trn_dataset = ImageDataset(\n",
    "    csv=\"../data/train_split.csv\",\n",
    "    img_dir=\"../data/train\",\n",
    "    transform=trn_transform,\n",
    "    transform_strong=strong_train_transform,\n",
    "    minority_set=minority_set,\n",
    "    has_label=True,            # (기본값 True라 생략해도 됨)\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    csv=\"../data/val_split.csv\",\n",
    "    img_dir=\"../data/train\",\n",
    "    transform=tst_transform,   # val은 test와 동일 파이프라인\n",
    "    has_label=True,\n",
    ")\n",
    "\n",
    "# ⚠️ sample_submission.csv에 target이 비어 있거나 NaN이면 int 변환 오류 발생 가능.\n",
    "# 안전하게 has_label=False로 두고 (img, id) 반환받자.\n",
    "tst_dataset = ImageDataset(\n",
    "    csv=\"../data/sample_submission.csv\",\n",
    "    img_dir=\"../data/test\",\n",
    "    transform=tst_transform,\n",
    "    has_label=False,           # ← 중요\n",
    ")\n",
    "\n",
    "print(len(trn_dataset), len(val_dataset), len(tst_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca983f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sampler ready] classes=17, min_class=37, max_class=80, samples=1256\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch, pandas as pd, numpy as np\n",
    "\n",
    "# === 0) 공통 하이퍼파라미터 별칭 ===\n",
    "BATCH_SIZE  = CFG[\"batch_size\"]\n",
    "NUM_WORKERS = CFG[\"num_workers\"] = min(4, os.cpu_count() or 1)\n",
    "PIN_MEMORY  = CFG[\"pin_memory\"] = True\n",
    "PERSISTENT  = CFG[\"persistent_workers\"] = True\n",
    "PREFETCH = 2\n",
    "\n",
    "# === 1) 샘플 가중치 계산: 반드시 trn_dataset과 같은 CSV 사용 ===\n",
    "trn_split_path = \"../data/train_split.csv\"   # ★ 여기 통일\n",
    "df_w = pd.read_csv(trn_split_path)\n",
    "\n",
    "counts   = df_w[\"target\"].value_counts().sort_index()\n",
    "n_class  = counts.shape[0]\n",
    "weight_map = counts.sum() / (n_class * counts)       # class별 weight = N / (K * n_c)\n",
    "sample_weights = df_w[\"target\"].map(weight_map).astype(\"float64\").values\n",
    "\n",
    "# 길이/라벨 일치 sanity check (선택)\n",
    "assert len(sample_weights) == len(trn_dataset), \"weights 길이와 trn_dataset 길이가 다릅니다.\"\n",
    "assert set(df_w[\"target\"].unique()) == set(range(n_class)) or len(set(df_w[\"target\"])) == n_class\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.from_numpy(sample_weights).to(dtype=torch.double),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# === 2) DataLoader ===\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,          # sampler 사용 시 shuffle=False\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=PERSISTENT if NUM_WORKERS > 0 else False,\n",
    "    drop_last=False           # BN 쓰면 True 고려(ConvNeXt는 LN이라 False도 OK)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=PERSISTENT if NUM_WORKERS > 0 else False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,            # 테스트는 순서 유지\n",
    "    num_workers=NUM_WORKERS,  # 0 고집할 이유 없으면 동일 값 사용 OK\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=PERSISTENT if NUM_WORKERS > 0 else False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[sampler ready] classes={n_class}, \"\n",
    "    f\"min_class={int(counts.min())}, max_class={int(counts.max())}, \"\n",
    "    f\"samples={len(sample_weights)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fad5b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4130ded938244e0a367664f9b06b61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=convnext_tiny, params=27.83M, img_size=448\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "device = CFG[\"device\"]\n",
    "\n",
    "# 1) 모델\n",
    "model = timm.create_model(\n",
    "    CFG[\"model_name\"],         # 예: \"convnext_tiny\"\n",
    "    pretrained=True,\n",
    "    num_classes=CFG[\"num_classes\"]\n",
    ").to(device)\n",
    "\n",
    "# 2) 손실함수\n",
    "# 현재 train에 WeightedRandomSampler를 쓰고 있으니, 과보정 방지 위해 처음엔 weight=None 권장\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.02)   # (0.00~0.05 사이 권장)\n",
    "\n",
    "# 3) 옵티마이저\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CFG[\"lr\"],\n",
    "    weight_decay=CFG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# 4) 스케줄러 (간단 버전: Cosine만)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CFG[\"epochs\"],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# 확인용\n",
    "n_params = sum(p.numel() for p in model.parameters())/1e6\n",
    "print(f\"model={CFG['model_name']}, params={n_params:.2f}M, img_size={CFG['img_size']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "547863bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=17, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e656cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: [torch.Size([4, 3, 448, 448]), torch.Size([4])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 임시 디버그용: 워커 0, 핀메모리 X\n",
    "dbg_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# 한 배치만 시도 → 여기서 터지면 실제 예외가 콘솔에 찍힘(FileNotFoundError, PIL 에러 등)\n",
    "batch = next(iter(dbg_loader))\n",
    "print(\"OK:\", [t.shape for t in batch if hasattr(t, \"shape\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33ef91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing files: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "missing = []\n",
    "for i in range(len(trn_dataset)):\n",
    "    name = str(trn_dataset.df.iloc[i][\"ID\"])\n",
    "    try:\n",
    "        path = trn_dataset._make_path(name)   # 우리가 만든 ImageDataset에 존재\n",
    "    except AttributeError:\n",
    "        # _make_path가 없다면 직접 구성\n",
    "        has_ext = name.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".gif\",\".webp\"))\n",
    "        fname = name if has_ext else f\"{name}.jpg\"\n",
    "        path = os.path.join(\"../data/train\", fname)\n",
    "    if not os.path.exists(path):\n",
    "        missing.append(path)\n",
    "        if len(missing) >= 10: break\n",
    "\n",
    "print(\"missing files:\", len(missing))\n",
    "print(\"\\n\".join(missing[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d31139fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/30] train_loss=0.8088  train_f1=0.7404  val_loss=0.7644  val_f1=0.7675  lr=0.000298\n",
      ">> best updated! val_f1=0.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/30] train_loss=0.6709  train_f1=0.8092  val_loss=0.6963  val_f1=0.7893  lr=0.000294\n",
      ">> best updated! val_f1=0.7893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/30] train_loss=0.5082  train_f1=0.8420  val_loss=0.6062  val_f1=0.8329  lr=0.000289\n",
      ">> best updated! val_f1=0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/30] train_loss=0.5082  train_f1=0.8654  val_loss=0.4874  val_f1=0.8493  lr=0.000283\n",
      ">> best updated! val_f1=0.8493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/30] train_loss=0.4168  train_f1=0.9017  val_loss=0.4049  val_f1=0.8750  lr=0.000275\n",
      ">> best updated! val_f1=0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/30] train_loss=0.3912  train_f1=0.9182  val_loss=0.4207  val_f1=0.8869  lr=0.000265\n",
      ">> best updated! val_f1=0.8869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/30] train_loss=0.3439  train_f1=0.9297  val_loss=0.4050  val_f1=0.9013  lr=0.000255\n",
      ">> best updated! val_f1=0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/30] train_loss=0.3183  train_f1=0.9391  val_loss=0.4625  val_f1=0.8830  lr=0.000243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/30] train_loss=0.3307  train_f1=0.9289  val_loss=0.4386  val_f1=0.8935  lr=0.000230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/30] train_loss=0.2886  train_f1=0.9494  val_loss=0.4352  val_f1=0.9005  lr=0.000217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/30] train_loss=0.2652  train_f1=0.9585  val_loss=0.4338  val_f1=0.8969  lr=0.000202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/30] train_loss=0.2389  train_f1=0.9674  val_loss=0.4094  val_f1=0.9090  lr=0.000187\n",
      ">> best updated! val_f1=0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/30] train_loss=0.2118  train_f1=0.9800  val_loss=0.4287  val_f1=0.8967  lr=0.000172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/30] train_loss=0.2208  train_f1=0.9761  val_loss=0.4130  val_f1=0.9188  lr=0.000156\n",
      ">> best updated! val_f1=0.9188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/30] train_loss=0.2051  train_f1=0.9791  val_loss=0.3922  val_f1=0.9260  lr=0.000141\n",
      ">> best updated! val_f1=0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/30] train_loss=0.1923  train_f1=0.9840  val_loss=0.3755  val_f1=0.9305  lr=0.000125\n",
      ">> best updated! val_f1=0.9305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/30] train_loss=0.2084  train_f1=0.9762  val_loss=0.3966  val_f1=0.8881  lr=0.000110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/30] train_loss=0.1735  train_f1=0.9922  val_loss=0.4400  val_f1=0.9234  lr=0.000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/30] train_loss=0.1673  train_f1=0.9969  val_loss=0.3545  val_f1=0.9391  lr=0.000081\n",
      ">> best updated! val_f1=0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/30] train_loss=0.1669  train_f1=0.9975  val_loss=0.4728  val_f1=0.9148  lr=0.000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/30] train_loss=0.1656  train_f1=0.9961  val_loss=0.4078  val_f1=0.9273  lr=0.000055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/30] train_loss=0.1615  train_f1=0.9961  val_loss=0.4077  val_f1=0.9286  lr=0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/30] train_loss=0.1518  train_f1=1.0000  val_loss=0.3900  val_f1=0.9345  lr=0.000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/30] train_loss=0.1566  train_f1=0.9992  val_loss=0.3834  val_f1=0.9378  lr=0.000024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/30] train_loss=0.1565  train_f1=0.9984  val_loss=0.3782  val_f1=0.9262  lr=0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/30] train_loss=0.1537  train_f1=0.9981  val_loss=0.3797  val_f1=0.9351  lr=0.000010\n",
      ">> early stop: no improvement for 7 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scaler   = GradScaler(enabled=CFG[\"use_amp\"])\n",
    "best_f1  = -1.0\n",
    "patience = 7          # 선택: 조기종료\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, CFG[\"epochs\"] + 1):\n",
    "    # 1) Train (여기서 scheduler는 epoch 끝에 내부에서 step 안 되게 하기)\n",
    "    tr = train_one_epoch(\n",
    "        trn_loader, model, optimizer, loss_fn, CFG[\"device\"],\n",
    "        scheduler=None,           # ← 전달하면 train_one_epoch 끝에서 step\n",
    "        scaler=scaler,\n",
    "        grad_accum_steps=CFG[\"grad_accum\"],\n",
    "        max_grad_norm=CFG[\"max_grad_norm\"],\n",
    "        use_amp=CFG[\"use_amp\"]\n",
    "    )\n",
    "\n",
    "    # 2) Validate\n",
    "    va = validate_one_epoch(\n",
    "        val_loader, model, loss_fn, CFG[\"device\"], return_probs=False\n",
    "    )\n",
    "\n",
    "    # ====== 스케줄러는 여기서 '한 번만' ======\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        # metric으로 제어: 보통 val_loss (mode='min') 또는 val_f1 (mode='max')\n",
    "        # 예) mode='max'로 만들었다면 va['val_f1'] 전달\n",
    "        scheduler.step(va['val_loss'])   # or va['val_f1']\n",
    "    else:\n",
    "        scheduler.step()                 # Cosine/Step/OneCycle 등은 인자 없이\n",
    "\n",
    "    curr_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"[{epoch:02d}/{CFG['epochs']}] \"\n",
    "          f\"train_loss={tr['train_loss']:.4f}  train_f1={tr['train_f1']:.4f}  \"\n",
    "          f\"val_loss={va['val_loss']:.4f}  val_f1={va['val_f1']:.4f}  lr={curr_lr:.6f}\")\n",
    "\n",
    "    if va[\"val_f1\"] > best_f1:\n",
    "        best_f1 = va[\"val_f1\"]; no_improve = 0\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"best_f1\": best_f1, \"cfg\": CFG},\n",
    "                   f\"best_{CFG['model_name']}_img{CFG['img_size']}.pth\")\n",
    "        print(f\">> best updated! val_f1={best_f1:.4f}\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\">> early stop: no improvement for {patience} epochs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b210199",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691be107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ckpt: best_convnext_tiny_img448.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=17, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 체크포인트 로드\n",
    "import torch, os\n",
    "\n",
    "device = CFG[\"device\"]\n",
    "\n",
    "# 네가 저장한 최신 포맷 (앞서 학습 루프에서 저장)\n",
    "ckpt_new = f\"best_{CFG['model_name']}_img{CFG['img_size']}.pth\"\n",
    "# 예전 파일명도 혹시 남아있을 수 있으니 fallback\n",
    "ckpt_old = \"best_resnet34_2.pth\"\n",
    "\n",
    "ckpt_path = ckpt_new if os.path.exists(ckpt_new) else ckpt_old\n",
    "print(\"loading ckpt:\", ckpt_path)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(\"[state_dict] missing:\", missing, \"unexpected:\", unexpected)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf19d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num predictions: 3140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ids, preds = [], []\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "for images, names in tqdm(tst_loader):\n",
    "    images = images.to(device, non_blocking=True)\n",
    "    logits = model(images)\n",
    "    pred = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    ids.extend(names)     # ImageDataset(has_label=False)에서 반환한 식별자\n",
    "    preds.extend(pred.tolist())\n",
    "\n",
    "print(\"num predictions:\", len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2467d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_list 길이 = 테스트 샘플 수 체크\n",
    "assert len(preds) == len(tst_dataset), \"예측 개수와 테스트셋 길이가 다릅니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5742cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID만 복사해서 target 채우기\n",
    "pred_df = tst_dataset.df[[\"ID\"]].copy()\n",
    "pred_df[\"target\"] = np.array(preds, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6e28b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 제출과 순서 일치 확인(강추)\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df[\"ID\"].values == pred_df[\"ID\"].values).all(), \"ID 순서가 다릅니다.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6737a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값/자료형 점검(선택)\n",
    "assert pred_df[\"target\"].between(0, 16).all(), \"라벨 범위(0~16) 확인!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65d8fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> code4_pred.csv\n"
     ]
    }
   ],
   "source": [
    "pred_df.to_csv(\"code4_pred.csv\", index=False)\n",
    "print(\"Saved -> code4_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f666f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
